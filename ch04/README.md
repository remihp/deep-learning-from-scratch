# ch04 NNの学習

## 4.1 データから学習する

【特徴量と機械学習のアプローチ】  
SIFT, SURF, HOG : CVの分野で有名  
これらを用いて機械学習(SVM, KNNなど)のアプローチで識別する

【NN】  
重要な特徴量までも機械が学習する

## 4.2 損失関数

- 2乗和誤差  
    $y_k$ : NNの出力, $t_k$ : 教師データ(正解データ), $K$ : データの次元数

$$
E=\frac{1}{2}\sum_k (y_k-t_k)^2
$$


- 交差エントロピー誤差  
    $t_k$ : one-hot表現

$$
E=-\sum_k t_k \log y_k
$$



- ミニバッチ学習 : 訓練データからある枚数だけ取り出しミニバッチごとに学習する   
    損失関数の和(データ数が$N$個)  

$$
E=-\frac{1}{N}\sum_n \sum_k t_{nk} \log y_{nk}
$$


## 4.5 学習アルゴリズムの実装

**確率的勾配降下法(SGD)**  


1. ミニバッチ  
    訓練データの中からランダムに一部のデータ(ミニバッチ)を選び出す  
    ミニバッチの損失関数の値を減らすことを考える

2. 勾配の算出  
    各重みパラメータの勾配を求める
    勾配は損失関数の値を最も減らす方向を示す

3. パラメータの更新  
    重みパラメータを勾配方向に微少量だけ更新する

4. 繰り返す  

    1-3を繰り返す
